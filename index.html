<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Abdulkadir Gokce</title>

  <meta name="author" content="Abdulkadir Gokce">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Abdulkadir Gokce
                  </p>
                  <p>
                    I am a first-year Ph.D. student in Computer Science at the 
                    <a href="https://www.epfl.ch/">Swiss Federal Institute of Technology Lausanne (EPFL)</a>, 
                    where I am advised by <a href="https://mschrimpf.altervista.org/">Martin Schrimpf</a>. 
                    My research focuses on building artificial models of perception. 
                  </p>
                  <p>
                    Previously, I earned my master's degree at <a href="https://www.epfl.ch/">EPFL</a> 
                    and my bachelor's in Electrical & Electronics Engineering and Mathematics at <a href="https://www.boun.edu.tr/">Bogazici University</a>. 
                    I was fortunate to gain research experience at <a href="https://neurotechnology.ethz.ch">ETH Zurich</a>, 
                    <a href="https://eggerbernhard.ch">MIT</a>, and <a href="https://www.ntu.edu.sg/erian">Nanyang Technological University (NTU)</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:abdulkadir.gokce AT epfl.ch">Email</a> &nbsp;/&nbsp;
                    <!-- <a href="data/CV.pdf">CV</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=jd-wZxkAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/akgokce">Github</a> &nbsp;/&nbsp;
                    <a href="https://x.com/akgokce0">Twitter</a> &nbsp;/&nbsp;
                    <a href="https://bsky.app/profile/akgokce.bsky.social">Bluesky</a> 
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/profile.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    My research investigates the extent to which current deep learning models can capture neural and behavioral signals, 
                    and aims to develop new modeling strategies that expand these capabilities. 
                    I am particularly focused on building scalable, multimodal models of human perception that integrate diverse neurocognitive datasets.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:16px;width:40%;vertical-align:middle">
                  <img src="images/scaling_primate_vvs.jpg" alt="scaling_primate_vvs_jpg" style="border-style: none" width="100%">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2411.05712">
                    <span class="papertitle">Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream
                    </span>
                  </a>
                  <br>
                  <strong>Abdulkadir Gokce</strong>,
                  Martin Schrimpf
                  <br>
                  <em>ICML</em>, 2025 &nbsp <font color="#C00000"><strong>[Spotlight, Top 3%]</strong></font>
                  <br>
                  <!-- <a href="">
                    <papertitle>[Website]</papertitle>
                  </a> -->
                  <a href="https://scaling-primate-vvs.epfl.ch">
                      <papertitle>[Project page]</papertitle>
                  </a>
                  <a href="https://openreview.net/forum?id=WxY61MmHYo">
                      <papertitle>[OpenReview]</papertitle>
                  </a>
                  <a href="https://arxiv.org/abs/2411.05712">
                      <papertitle>[arXiv]</papertitle>
                  </a>
                  <a href="https://github.com/epflneuroailab/scaling-primate-vvs">
                      <papertitle>[Code]</papertitle>
                  </a>
                  <p></p>
                  <p>
                    We systematically explored scaling laws for primate vision models and discovered that neural alignment stops improving beyond a certain scale, even though behavior keeps aligning better.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:40%;vertical-align:middle">
                  <img src="images/fragmented_objects.jpg" alt="fragmented_objects_jpg" style="border-style: none" width="100%">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2504.05253">
                    <span class="papertitle">Contour Integration Underlies Human-Like Vision
                    </span>
                  </a>
                  <br>
                  Ben Lonnqvist, 
                  Elsa Scialom*,
                  <strong>Abdulkadir Gokce*</strong>,
                  Zehra Merchant, 
                  Michael Herzog,
                  Martin Schrimpf
                  <br>
                  <em>ICML</em>, 2025 &nbsp
                  <br>
                  <!-- <a href="">
                    <papertitle>[Website]</papertitle>
                  </a> -->
                  <a href="https://openreview.net/forum?id=ftR9OuiUJA">
                      <papertitle>[OpenReview]</papertitle>
                  </a>
                  <a href="https://arxiv.org/abs/2504.05253">
                      <papertitle>[arXiv]</papertitle>
                  </a>
                  <!-- <a href="">
                      <papertitle>[Code]</papertitle>
                  </a> -->
                  <p></p>
                  <p>
                    We find that contour integration, a core feature of human object recognition, emerges in models only at large scales and correlates with improved shape bias.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:40%;vertical-align:middle">
                  <img src="images/babyllama_2024.jpg" alt="babyllama_2024_jpg" style="border-style: none" width="100%">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2411.00828">
                    <span class="papertitle">Dreaming Out Loud: A Self-Synthesis Approach For Training Vision-Language Models With Developmentally Plausible Data
                    </span>
                  </a>
                  <br>
                  Badr AlKhamissi*, 
                  Yingtian Tang*, 
                  <strong>Abdulkadir Gokce*</strong>, 
                  Johannes Mehrer, 
                  Martin Schrimpf
                  <br>
                  <em>BabyLM Challenge</em>,  at <em>CoNLL</em> 2024 &nbsp
                  <br>
                  <!-- <a href="">
                    <papertitle>[Website]</papertitle>
                  </a> -->
                  <a href="https://openreview.net/forum?id=70Hkypl2gx">
                      <papertitle>[OpenReview]</papertitle>
                    </a>
                  <a href="https://arxiv.org/abs/2411.00828">
                      <papertitle>[arXiv]</papertitle>
                    </a>
                  <!-- <a href="">
                      <papertitle>[Code]</papertitle>
                  </a> -->
                  <p></p>
                  <p>
                    Inspired by human cognitive development, our BabyLLaMA model learns language and vision jointly through a self-synthesis loop, generating its own training data from unlabeled images.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:40%;vertical-align:middle">
                  <img src="images/endol2h.jpg" alt="endol2h_jpg" style="border-style: none" width="100%">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2002.05459">
                    <span class="papertitle">EndoL2H: Deep Super-Resolution for Capsule Endoscopy
                    </span>
                  </a>
                  <br>
                  Yasin Almalioglu, 
                  Kutsev Bengisu Ozyoruk, 
                  <strong>Abdulkadir Gokce</strong>, 
                  Kagan Incetan, 
                  Guliz Irem Gokceler, 
                  Muhammed Ali Simsek, 
                  Kivanc Ararat, 
                  Richard J Chen, 
                  Nicholas J Durr, 
                  Faisal Mahmood, 
                  Mehmet Turan
                  <br>
                  <em>IEEE Transactions on Medical Imaging</em> &nbsp
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9167261">
                    <papertitle>[Paper]</papertitle>
                  </a>
                  <a href="https://arxiv.org/abs/2002.05459">
                      <papertitle>[arXiv]</papertitle>
                    </a>
                  <a href="https://github.com/CapsuleEndoscope/EndoL2H">
                      <papertitle>[Code]</papertitle>
                  </a>
                  <p></p>
                  <p>
                    EndoL2H enhances capsule endoscopy images up to 12x using a spatial attention-guided GAN, outperforming existing methods in perceptual quality and clinical relevance.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>



    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:center;font-size:small;">
            Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>.
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>